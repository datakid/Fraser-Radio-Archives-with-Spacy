{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language analysis using Python and Spacy\n",
    "\n",
    "We will use the Python module Spacy to analize the Fraser Radio archives. On the way we will learn a couple of other tricks to make our lives easier.\n",
    "\n",
    "\n",
    "```\n",
    "wget https://archives.unimelb.edu.au/__data/assets/text_file/0006/1717746/UMA_Fraser_Radio_Talks.zip\n",
    "pip3 install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "That gets us ready to do the linguistic work.\n",
    "\n",
    "But before that, we need to read the data in and clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UDS2013680-373-full.txt', 'UDS2013680-240-full.txt', 'UDS2013680-720-full.txt']\n"
     ]
    }
   ],
   "source": [
    "# Can we see the files? Yes - here are the titles. \n",
    "import os\n",
    "files = os.listdir('UMA_Fraser_Radio_Talks')\n",
    "print(files[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are txt files - the file type that traditionally contains text. Great, let's look inside them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x84 in position 2857: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d6cf20c4bf50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Can we read the files as text? Yes... No.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UMA_Fraser_Radio_Talks'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x84 in position 2857: invalid start byte"
     ]
    }
   ],
   "source": [
    "# Can we read the files as text? Yes... No.\n",
    "f = open(os.path.join('UMA_Fraser_Radio_Talks', files[0]), \"r\")\n",
    "text = f.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UnicodeDecode Errors - character sets, unicode and automation\n",
    "\n",
    "UnicodeDecode errors will be frequent in Python version 3 - they have changed how they represent human language within the Python language. While it's annoying - and slightly more work - the solution we implement below is relatively quick and solves the problem *regardless of the language of the documents we are researching*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!--start metadata-->\\r\\nTitle: Closure of the customs inwards and outwards shipping register for Portland\\r\\nDescription: radio talk\\r\\nDate: c1957\\r\\nCollection: John Malcolm Fraser, 2007.0023\\r\\nCollection URI: http://gallery.its.unimelb.edu.au/imu/imu.php?request=load&irn=115190&ecatalogue=on&view=details\\r\\nFormat: Uncorrected OCR text\\r\\n<!--end metadata-->\\r\\n\\r\\n\\r\\ns.\\r\\n                                NEW ITEM.\\r\\n\\r\\n\\r\\n                  A most unusual ceremony took place at Parliament \\r\\n      House, Canberra, on Thursday, 14th November, 1957, when the \\r\\n      Einister of State for Customs and Excise, Senator, the Hon. \\r\\n      Denham Henty closed the Customs Inward6and OutwardsShipping \\r\\n      Register, for theport of Portland, Victoria, just 100 years\\r\\n      after the first entry was made in it.\\r\\n                  These Registers record shipping movements into \\r\\n      and out of the ports of the Commonwealth and show such things \\r\\n       as tonnages, number of passengers and crew, and so on.\\r\\n                  In this instance, the first entry in the\\r\\n       Portland Register was for the \"Frances Henty\", a Clipper \\r\\n       of 432 tons owned by Stephen Henty, the Shipping Agents being\\r\\n       Henty and Co.\\r\\n                   Stephen Henty, one of the members of the famous\\r\\n       Australian pioneering family, came out from Sussex with his \\r\\n       brothers, James and John at the request of their parents,\\r\\n       Thomas and Frances Henty.   They first of all took up land \\r\\n       grants at the Swan River Settlement, Fremantle in 1829. \\r\\n                   Senator Henty is a direct descendant of Thomas\\r\\n        and Frances Henty, through their son James, who led the \\r\\n        expedition to the new colony.\\r\\n                   Unfortunately, the land grants given to the \\r\\n        brothers fn the colony were of poor grazing quality and \\r\\n        complaints to the Governor, Sir James Stirling, were of\\r\\n        little avail.   LventuallY, in 1831, James Henty decided \\r\\n        to sell up the family holdings and move to Launceston.\\r\\n        He there founded the rterallahtfirm of Henty and Co. Stephen \\r\\n        followed his brother to Launceston in 1832 and eventually\\r\\n        settled at Portland, Victoria, late in 1836.\\r\\n\\x0c\\r\\n                              2.\\r\\n\\r\\n\\r\\n              Now that the Register has been closed, it will \\r\\nbe retained as an historical chronicle of early shipping\\r\\nmovements in Australia.        It also has a practical use in \\r\\nthat the Register is used to relate all shipping documents \\r\\nand passenger lists to particular vessels and for these \\r\\npurposes, reference will be made to the Register as \\r\\nrequired, for many years to come.\\r\\n\\r\\n\\r\\n                 Kt,                               lA\\r\\n           /.1\\r\\n                                           \"4.A.0          /i\\r\\n                              ,4      744,4---^\\x84 pt/t-te\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                        \\'Llpitft tfte Co fur   enta\\r\\n\\r\\n                       5enatot Tertfictm 5Centj.\\r\\n\\x0c\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "# Can we read the files as binary text files? Yes!\n",
    "f = open(os.path.join('UMA_Fraser_Radio_Talks', files[0]), 'rb')\n",
    "text = f.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that looks better. What was that **b** we added to the open path? That told python to open the file to (r)ead - but to read the file as a byte stream instead of as text<sup>1</sup>. That's handy, but why and is it useful? \n",
    "\n",
    "<sup>1</sup> with the default ASCII encoding. This is important later.\n",
    "\n",
    "Well, we can now read the file - and we can see the section near the bottom of the file that caused the issue. That second box shows another example - although in this case it's valid ASCII.\n",
    "\n",
    "\n",
    "![Hex Chars](imgs/non-text-characters_small.png \"Hex Chars\")\n",
    "\n",
    "\n",
    "What we are seeing here is a data quality issue. This is very common, especially with text that has been scanned from PDFs. The image to text transfer will make a best guess, and in this case it's guessed an unusual character. If you take a look at the [original pdf](https://digitised-collections.unimelb.edu.au/bitstream/handle/11343/40335/312821_2007-0023-0372.pdf), you can see that this is because Mr Cain had made hand written notes on the bottom of his talk. \n",
    "\n",
    "The character combination **\\x** is a restriction within [ASCII](https://en.wikipedia.org/wiki/ASCII) text files - it is a leading indicator used to encode characters as bytes. For instance \\x20 is the space character, \\x3a is the colon character (:) and \\x41 is capital A. \n",
    "\n",
    "We wont go into character encoding now, but we will show into how to solve the problem. The problem starts with **\\x84** being an invalid ASCII code - **\\x7F** (the \"Delete\" character) is as high as ASCII goes. \n",
    "\n",
    "Why do we want to solve this - why not just import the files as \"bytestreams\"? The primary reason is that text manipulation in python is powerful and easy to use - but it only works on text, not bytestreams. \n",
    "\n",
    "Thankfully, those that came before us have solved the problem of guessing the encoding and have written a Python modue called Chardet (\"character detect\") to solve it. How did I know to use this software? I did a search for \"python determine character set\".\n",
    "\n",
    "### Character Set Detection\n",
    "\n",
    "Let's install chardet and use that to get an idea for the probable character encoding. This code is copy and pasted from [the Chardet documentation](https://chardet.readthedocs.io/en/latest/usage.html#example-detecting-encodings-of-multiple-files) and slightly altered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UDS2013680-546-full.txt :  {'encoding': 'Windows-1252', 'confidence': 0.73, 'language': ''}\n",
      "UDS2013680-297-full.txt :  {'encoding': 'Windows-1252', 'confidence': 0.73, 'language': ''}\n",
      "UDS2013680-639-full.txt :  {'encoding': 'ascii', 'confidence': 1.0, 'language': ''}\n",
      "UDS2013680-206-full.txt :  {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n",
      "UDS2013680-583-full.txt :  {'encoding': 'Windows-1252', 'confidence': 0.73, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "from chardet.universaldetector import UniversalDetector\n",
    "\n",
    "detector = UniversalDetector()\n",
    "\n",
    "for f in files[20:25]:\n",
    "    filename=os.path.join('UMA_Fraser_Radio_Talks',f)\n",
    "    detector.reset()\n",
    "    for line in open(filename, 'rb'):\n",
    "        detector.feed(line)\n",
    "        if detector.done: break\n",
    "    detector.close()\n",
    "    print(f, \": \", detector.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally all text files for what are known English Language texts would just be in English, but as you can see the data isn't as clean as we would like. In fact, there are at least three detected character encodings - **Windows-1252**, **ASCII** and **ISO-8859-1**. \n",
    "\n",
    "We will need to deal with this on a file by file basis - but thankfully we don't actually need to know what each of those encodings represents. \n",
    "\n",
    "The reality is that - like with our first file - the problem is dirty data problem. A single misreaded character by the Optical Character Recognition software and everything is a mess.\n",
    "\n",
    "Let's turn the above into a function that just returns the encoding so we can use it repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_charset(filename):\n",
    "\n",
    "    import chardet\n",
    "    from chardet.universaldetector import UniversalDetector\n",
    "\n",
    "    detector = UniversalDetector()\n",
    "    detector.reset()\n",
    "    for line in open(filename, 'rb'):\n",
    "        detector.feed(line)\n",
    "        if detector.done: break\n",
    "    detector.close()\n",
    "\n",
    "    return detector.result['encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!--start metadata-->\n",
      "Title: Closure of the customs inwards and outwards shipping register for Portland\n",
      "Description: radio talk\n",
      "Date: c1957\n",
      "Collection: John Malcolm Fraser, 2007.0023\n",
      "Collection URI: http://gallery.its.unimelb.edu.au/imu/imu.php?request=load&irn=115190&ecatalogue=on&view=details\n",
      "Format: Uncorrected OCR text\n",
      "<!--end metadata-->\n",
      "\n",
      "\n",
      "s.\n",
      "                                NEW ITEM.\n",
      "\n",
      "\n",
      "                  A most unusual ceremony took place at Parliament \n",
      "      House, Canberra, on Thursday, 14th November, 1957, when the \n",
      "      Einister of State for Customs and Excise, Senator, the Hon. \n",
      "      Denham Henty closed the Customs Inward6and OutwardsShipping \n",
      "      Register, for theport of Portland, Victoria, just 100 years\n",
      "      after the first entry was made in it.\n",
      "                  These Registers record shipping movements into \n",
      "      and out of the ports of the Commonwealth and show such things \n",
      "       as tonnages, number of passengers and crew, and so on.\n",
      "                  In this instance, the first entry in the\n",
      "       Portland Register was for the \"Frances Henty\", a Clipper \n",
      "       of 432 tons owned by Stephen Henty, the Shipping Agents being\n",
      "       Henty and Co.\n",
      "                   Stephen Henty, one of the members of the famous\n",
      "       Australian pioneering family, came out from Sussex with his \n",
      "       brothers, James and John at the request of their parents,\n",
      "       Thomas and Frances Henty.   They first of all took up land \n",
      "       grants at the Swan River Settlement, Fremantle in 1829. \n",
      "                   Senator Henty is a direct descendant of Thomas\n",
      "        and Frances Henty, through their son James, who led the \n",
      "        expedition to the new colony.\n",
      "                   Unfortunately, the land grants given to the \n",
      "        brothers fn the colony were of poor grazing quality and \n",
      "        complaints to the Governor, Sir James Stirling, were of\n",
      "        little avail.   LventuallY, in 1831, James Henty decided \n",
      "        to sell up the family holdings and move to Launceston.\n",
      "        He there founded the rterallahtfirm of Henty and Co. Stephen \n",
      "        followed his brother to Launceston in 1832 and eventually\n",
      "        settled at Portland, Victoria, late in 1836.\n",
      "\f",
      "\n",
      "                              2.\n",
      "\n",
      "\n",
      "              Now that the Register has been closed, it will \n",
      "be retained as an historical chronicle of early shipping\n",
      "movements in Australia.        It also has a practical use in \n",
      "that the Register is used to relate all shipping documents \n",
      "and passenger lists to particular vessels and for these \n",
      "purposes, reference will be made to the Register as \n",
      "required, for many years to come.\n",
      "\n",
      "\n",
      "                 Kt,                               lA\n",
      "           /.1\n",
      "                                           \"4.A.0          /i\n",
      "                              ,4      744,4---^â€ž pt/t-te\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        'Llpitft tfte Co fur   enta\n",
      "\n",
      "                       5enatot Tertfictm 5Centj.\n",
      "\f",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = os.path.join('UMA_Fraser_Radio_Talks', files[0])\n",
    "encoding = get_charset(filename)\n",
    "f = open(filename, 'r', encoding=encoding)\n",
    "text = f.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Data\n",
    "\n",
    "Now that looks much better. \n",
    "\n",
    "Let's keep cleaning it up. Looks like we can remove the metadata from the top. We could put it into a database or a dictionary for later use if we want. For the moment, let's just split it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!--start metadata-->\\nTitle: Closure of the customs inwards and outwards shipping register for Portland\\nDescription: radio talk\\nDate: c1957\\nCollection: John Malcolm Fraser, 2007.0023\\nCollection URI: http://gallery.its.unimelb.edu.au/imu/imu.php?request=load&irn=115190&ecatalogue=on&view=details\\nFormat: Uncorrected OCR text\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = text.split(\"<!--end metadata-->\")\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Let's see if we can't start making some sense of what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* <!--start metadata-->\n",
      "* Title: Closure of the customs inwards and outwards shipping register for Portland\n",
      "* Description: radio talk\n",
      "* Date: c1957\n",
      "* Collection: John Malcolm Fraser, 2007.0023\n",
      "* Collection URI: http://gallery.its.unimelb.edu.au/imu/imu.php?request=load&irn=115190&ecatalogue=on&view=details\n",
      "* Format: Uncorrected OCR text\n",
      "* \n"
     ]
    }
   ],
   "source": [
    "# split into lines, add '*' to the start of each line\n",
    "# \\n is a newline character\n",
    "for line in data[0].split('\\n'):\n",
    "    print('*', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Title: Closure of the customs inwards and outwards shipping register for Portland\n",
      "* Description: radio talk\n",
      "* Date: c1957\n",
      "* Collection: John Malcolm Fraser, 2007.0023\n",
      "* Collection URI: http://gallery.its.unimelb.edu.au/imu/imu.php?request=load&irn=115190&ecatalogue=on&view=details\n",
      "* Format: Uncorrected OCR text\n"
     ]
    }
   ],
   "source": [
    "# skip empty lines and any line that starts with '<'\n",
    "for line in data[0].split('\\n'):\n",
    "    if not line:\n",
    "        continue\n",
    "    if line.startswith('<'):\n",
    "        continue\n",
    "    print('*', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* ['Title', ' Closure of the customs inwards and outwards shipping register for Portland']\n",
      "* ['Description', ' radio talk']\n",
      "* ['Date', ' c1957']\n",
      "* ['Collection', ' John Malcolm Fraser, 2007.0023']\n",
      "* ['Collection URI', ' http', '//gallery.its.unimelb.edu.au/imu/imu.php?request=load&irn=115190&ecatalogue=on&view=details']\n",
      "* ['Format', ' Uncorrected OCR text']\n"
     ]
    }
   ],
   "source": [
    "# split the metadata items on ':' so that we can interrogate each one\n",
    "for line in data[0].split('\\n'):\n",
    "    if not line:\n",
    "        continue\n",
    "    if line[0] == '<':\n",
    "        continue\n",
    "    element = line.split(':')\n",
    "    print('*', element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* ['Title', ' Closure of the customs inwards and outwards shipping register for Portland']\n",
      "* ['Description', ' radio talk']\n",
      "* ['Date', ' c1957']\n",
      "* ['Collection', ' John Malcolm Fraser, 2007.0023']\n",
      "* ['Collection URI', ' http://gallery.its.unimelb.edu.au/imu/imu.php?request=load&irn=115190&ecatalogue=on&view=details']\n",
      "* ['Format', ' Uncorrected OCR text']\n"
     ]
    }
   ],
   "source": [
    "# actually, only split on the first colon\n",
    "for line in data[0].split('\\n'):\n",
    "    if not line:\n",
    "        continue\n",
    "    if line[0] == '<':\n",
    "        continue\n",
    "    element = line.split(':', 1)\n",
    "    print('*', element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Let's put it into a dictionary so we can use it later. \n",
    "\n",
    "The point of dictionaries is to store a key (the word) and a value (the count). When you ask for the key, you get its value.\n",
    "\n",
    "Notice that you use curly braces for dictionaries, but square brackets for lists.\n",
    "\n",
    "Dictionaries are a great way to work with the metadata in our corpus. Let's build a dictionary called metadata:\n",
    "\n",
    "Your first line will look like this:\n",
    "\n",
    "  metadata = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': ' Closure of the customs inwards and outwards shipping register for Portland', 'Description': ' radio talk', 'Date': ' c1957', 'Collection': ' John Malcolm Fraser, 2007.0023', 'Collection URI': ' http://gallery.its.unimelb.edu.au/imu/imu.php?request=load&irn=115190&ecatalogue=on&view=details', 'Format': ' Uncorrected OCR text'}\n"
     ]
    }
   ],
   "source": [
    "metadata = {}\n",
    "for line in data[0].split('\\n'):\n",
    "    if not line:\n",
    "        continue\n",
    "    if line[0] == '<':\n",
    "        continue\n",
    "    element = line.split(':', 1)\n",
    "    metadata[element[0]] = element[-1]\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " c1957\n"
     ]
    }
   ],
   "source": [
    "print(metadata['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn that into a function as well - we will be coming back to it for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_metadata(text):\n",
    "    metadata = {}\n",
    "    for line in text.split('\\n'):\n",
    "        if not line:\n",
    "            continue\n",
    "        if line[0] == '<':\n",
    "            continue\n",
    "        element = line.split(':', 1)\n",
    "        metadata[element[0]] = element[-1].strip(' ')\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': 'Closure of the customs inwards and outwards shipping register for Portland', 'Description': 'radio talk', 'Date': 'c1957', 'Collection': 'John Malcolm Fraser, 2007.0023', 'Collection URI': 'http://gallery.its.unimelb.edu.au/imu/imu.php?request=load&irn=115190&ecatalogue=on&view=details', 'Format': 'Uncorrected OCR text'}\n"
     ]
    }
   ],
   "source": [
    "md = parse_metadata(data[0])\n",
    "print(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata collection\n",
    "Now that we have all the tools we need to collect each file's metadata, let's do put it into a data structure so we can do some analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file is UMA_Fraser_Radio_Talks/UDS2013680-152-full.txt  and it's chardet data is None\n"
     ]
    }
   ],
   "source": [
    "fraser_talks_metadata = {}\n",
    "\n",
    "for file in os.listdir('UMA_Fraser_Radio_Talks'):\n",
    "    # If anything goes wrong, we will know which files to look at.\n",
    "    try:\n",
    "        filename = os.path.join('UMA_Fraser_Radio_Talks', file)\n",
    "        encoding = get_charset(filename)\n",
    "        text = open(filename, 'r', encoding=encoding).read()\n",
    "    except:\n",
    "        print(\"file is\", filename, \" and it's chardet data is\", get_charset(filename))\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    #split text of file on 'end metadata'\n",
    "    data = text.split(\"<!--end metadata-->\")\n",
    "    \n",
    "    #parse metadata using previously defined function \"parse_metadata\"\n",
    "    metadata = parse_metadata(data[0])\n",
    "    talk_data = data[1]\n",
    "      \n",
    "    fraser_talks_metadata[file]={'metadata':metadata, 'talk_data':talk_data}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Every error will be fixed. \n",
    "\n",
    "Something is wrong with that file. We will need to open it up to take a look. When we [open the file in jupyter notebook](UMA_Fraser_Radio_Talks/UDS2013680-152-full.txt) we can see at the very bottom two odd looking characters.\n",
    "\n",
    "![](imgs/weird_characters.png \"asda\") \n",
    "             \n",
    "What we have found is a document with conflicted information about what character encoding it is - those two characters are usual (on inspection, they are 0xC (the Form Feed character) and 0xDC (unknown). \n",
    "\n",
    "Since it's only one file and the process for discovery is nerdy and tedious, we will re-run the above with what professionals call [a *hack*](workingprocess.ipynb). I will put in a one time exception for this single file. You will not pass a PhD with hacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Title': 'Portland wool sales\\r',\n",
       " 'Description': 'press statement\\r',\n",
       " 'Date': '26/04/1964\\r',\n",
       " 'Collection': 'John Malcolm Fraser, 2007.0023\\r',\n",
       " 'Collection URI': 'http://gallery.its.unimelb.edu.au/imu/imu.php?request=load&irn=115190&ecatalogue=on&view=details\\r',\n",
       " 'Format': 'Uncorrected OCR text\\r'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraser_talks_metadata = {}\n",
    "\n",
    "for file in os.listdir('UMA_Fraser_Radio_Talks'):\n",
    "    # If anything goes wrong, we will know which files to look at.\n",
    "    try:\n",
    "        filename = os.path.join('UMA_Fraser_Radio_Talks', file)\n",
    "        encoding = get_charset(filename)\n",
    "        text = open(filename, 'r', encoding=encoding).read()\n",
    "    except:\n",
    "        # Special case: open the file in as binary, only read the first 2650 bytes\n",
    "        # once they are read, decode the binary as ascii\n",
    "        file_handle = open(filename, 'rb')\n",
    "        text = file_handle.read(2650).decode('ascii')\n",
    "            \n",
    "    #split text of file on 'end metadata'\n",
    "    data = text.split(\"<!--end metadata-->\")\n",
    "    \n",
    "    #parse metadata using previously defined function \"parse_metadata\"\n",
    "    metadata = parse_metadata(data[0])\n",
    "    talk_data = data[1]\n",
    "      \n",
    "    fraser_talks_metadata[file]={'metadata':metadata, 'talk_data':talk_data}\n",
    "\n",
    "fraser_talks_metadata['UDS2013680-152-full.txt']['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'1957': 37,\n",
       "         '1967': 92,\n",
       "         '1981': 23,\n",
       "         '1964': 44,\n",
       "         '1970': 36,\n",
       "         '1965': 39,\n",
       "         '1961': 41,\n",
       "         '1959': 41,\n",
       "         '1963': 48,\n",
       "         '1968': 26,\n",
       "         '1958': 33,\n",
       "         '1954': 24,\n",
       "         '1973': 33,\n",
       "         '1971': 27,\n",
       "         '1974': 19,\n",
       "         '1956': 16,\n",
       "         '1982': 23,\n",
       "         '1960': 41,\n",
       "         '1972': 42,\n",
       "         '1975': 40,\n",
       "         '1969': 15,\n",
       "         '1962': 31,\n",
       "         '1966': 3})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "fraser_talks_metadata.keys()\n",
    "\n",
    "dates = []\n",
    "\n",
    "for file_id in fraser_talks_metadata:\n",
    "    date = fraser_talks_metadata[file_id]['metadata']['Date']\n",
    "    if date.startswith('c'):\n",
    "    # date format cyyyy\n",
    "        year = date[1:]\n",
    "    elif len(date) == 10:\n",
    "    # date format dd/mm/yyyy\n",
    "        year = date[6:]\n",
    "    elif len(date) == 9:\n",
    "    # date format d/mm/yyyy\n",
    "        year = date[5:]\n",
    "    if len(year) == 5:\n",
    "    # pesky space in 1969\n",
    "        year = year.lstrip()\n",
    "    dates.append(year)\n",
    "        \n",
    "Counter(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we order those years? Probably. But at the moment they are strings, but we will want to order them as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('1954', 24),\n",
       "             ('1956', 16),\n",
       "             ('1957', 37),\n",
       "             ('1958', 33),\n",
       "             ('1959', 41),\n",
       "             ('1960', 41),\n",
       "             ('1961', 41),\n",
       "             ('1962', 31),\n",
       "             ('1963', 48),\n",
       "             ('1964', 44),\n",
       "             ('1965', 39),\n",
       "             ('1966', 3),\n",
       "             ('1967', 92),\n",
       "             ('1968', 26),\n",
       "             ('1969', 15),\n",
       "             ('1970', 36),\n",
       "             ('1971', 27),\n",
       "             ('1972', 42),\n",
       "             ('1973', 33),\n",
       "             ('1974', 19),\n",
       "             ('1975', 40),\n",
       "             ('1981', 23),\n",
       "             ('1982', 23)])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "data_summary = OrderedDict(sorted(Counter(dates).items(), key=lambda t: t))\n",
    "data_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO describe splicing arrays and strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO plot that distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#data_summary = Counter(dates)\n",
    "\n",
    "# we make a bar graph, the y axis will be the values in the data_summary\n",
    "plt.bar(range(len(data_summary)), list(data_summary.values()),align='center')\n",
    "\n",
    "# we add in the x axis details - and turn the years around so they fit\n",
    "plt.xticks(range(len(data_summary)), list(data_summary.keys()),rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. That looks great. Right away we can see that something momentous is happening in 1966 and 1967 - the lowest and highest number of talks respectively. If we look at his Wikipedia page, we can see that '66 is the year that he becomes a Government minister for the first time - and it's the Ministry for the Army as we enter the Vietnam War. \n",
    "\n",
    "Those records will be interesting to look at more closely later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
